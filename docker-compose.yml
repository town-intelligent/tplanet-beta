# TPlanet Deploy - Base Docker Compose
#
# Usage:
#   Beta:         docker compose -f docker-compose.yml -f docker-compose.beta.yml up -d
#   Stable:       docker compose -f docker-compose.yml -f docker-compose.stable.yml up -d
#   Multi-tenant: docker compose -f docker-compose.yml -f docker-compose.multi-tenant.yml up -d
#
# Prerequisites:
#   Run ./setup.sh to clone all apps

services:
  # Frontend - React + Vite
  frontend:
    build:
      context: ./apps/tplanet-AI
      dockerfile: Dockerfile
    volumes:
      - ./apps/tplanet-AI:/app
    networks:
      - tplanet-net

  # Backend - Django
  backend:
    build:
      context: ./apps/tplanet-daemon
      dockerfile: Dockerfile
    volumes:
      - ./apps/tplanet-daemon:/server
    depends_on:
      - db
    networks:
      - tplanet-net

  # Database - PostgreSQL
  db:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: ${DB_NAME:-tplanet}
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-postgres}
    volumes:
      - db_data:/var/lib/postgresql/data
    networks:
      - tplanet-net

  # Ollama - LLM Server
  ollama:
    image: ollama/ollama:0.3.12
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_PARALLEL=1
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - tplanet-net

  # Ollama Gateway - Proxy for Ollama
  ollama-gateway:
    build:
      context: ./apps/ollama-gateway/gateway
      dockerfile: Dockerfile
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - DEFAULT_MODEL=${OLLAMA_MODEL:-qwen2:7b-instruct}
      # Allow routing "openai/*" models through the gateway when configured
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8082/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - tplanet-net

  # LLMTwins - AI RAG & Agent Service
  llmtwins:
    build:
      context: ./apps/LLMTwins
      dockerfile: Dockerfile
    volumes:
      - ./apps/LLMTwins:/app
    environment:
      - OLLAMA_BASE_URL=http://ollama-gateway:8082
      # Required for models like "openai/gpt-4o-mini" (used by the AI secretary UI)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    depends_on:
      - ollama-gateway
    networks:
      - tplanet-net

  # Multi-tenant LLMTwins Wrapper
  llmtwins-wrapper:
    build:
      context: ./packages/multi-tenant/llmtwins_wrapper
    environment:
      - LLMTWINS_BASE_URL=http://llmtwins:8002
      - DEFAULT_TENANT=default
      - TENANT_HEADER=X-Tenant-ID
    depends_on:
      - llmtwins
    networks:
      - tplanet-net

volumes:
  db_data:
  ollama_data:

networks:
  tplanet-net:
    driver: bridge
